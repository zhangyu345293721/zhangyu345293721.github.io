---
title: "QLORA: Efficient Finetuning of Quantized LLMs"
collection: talks
permalink: /talks/lora
excerpt: '微调大型语言模型（LLMs）是提高性能和调整行为的有效方法。然而，对于非常大的模型来说，微调成本非常高昂，需要大量的GPU内存。传统的量化方法虽然可以减小模型的内存占用，但只适用于推理阶段，无法在训练过程中有效减少内存需求，这对于大型模型的微调构成了严重的限制。因此，当前研究面临着在保持性能的同时降低微调过程中的内存需求的挑战。'
date: 2025-01-31
---


> 在⼈⼯智能领域，⽆监督语⾔模型（Language Models, LMs）的发展已经达到了令⼈惊叹的⽔平，这些模型能够在⼴泛的数据集上进⾏预训练，学习到丰富的世界知识和⼀定的推理能⼒。然⽽，如何精确控制这些模型的⾏为， 使其按照⼈类的偏好和⽬标⾏动，⼀直是⼀个难题。这主要是因为这些模型的训练完全是⽆监督的，它们从⼈类⽣成的数据中学习，⽽这些数据背后的⽬标、优先级和技能⽔平五花⼋⻔。例如，我们希望⼈⼯智能编程助⼿能够理解常⻅的编程错误以便纠正它们，但在⽣成代码时，我们⼜希望模型能偏向于它训练数据中的⾼质量编码能⼒，即使这种能⼒可能相对罕⻅。
<br/>

## 1. 摘要解读

### QLORA论文的摘要篇幅相对较长，选取需要部分学习，通过摘要可以获得如下信息：

**方法介绍**：QLORA是一种高效的微调方法，用于在单个48GB的GPU上微调拥有65B参数的模型，并保持了16位微调任务的性能。
**方法细节**：QLORA使用了一个冻结的4位量化的预训练语言模型和低秩适配器（LoRA）来进行微调。
**性能结果**：研究者的最佳模型系列被命名为Guanaco，在Vicuna基准测试中表现出色，达到了99.3%的ChatGPT性能水平，只需要在单个GPU上进行24小时的微调。
**内存节省创新**：QLORA引入了创新性的方法来节省内存，包括新的数据类型（4位NormalFloat）、双量化以减少内存占用以及分页优化器来管理内存峰值。
**微调规模**：研究者使用QLORA来微调了1000多个模型，并进行了详细的性能分析，涵盖了8个指令数据集、不同的模型类型和规模。
**性能优势**：结果显示，QLORA微调在小型高质量数据集上能够达到最先进的性能，即使使用比以前的最先进模型更小的模型也能实现。

<img src='/images/lora_1.png'>


- 背景介绍：微调大型语言模型（LLMs）是提高性能和调整行为的有效方法。然而，对于非常大的模型来说，微调成本非常高昂，需要大量的GPU内存。传统的量化方法虽然可以减小模型的内存占用，但只适用于推理阶段，无法在训练过程中有效减少内存需求，这对于大型模型的微调构成了严重的限制。因此，当前研究面临着在保持性能的同时降低微调过程中的内存需求的挑战。
- QLORA方法：QQLORA是一种基于LoRA（Low-rank Adapter）的微调方法，其中适配器位于每个网络层。通过一系列创新工作，如引入4位NormalFloat、双量化和Paged Optimizers等方法，QLORA对LoRA方法进行了更好的调优，降低内存使用同时保持性能。通过在每个网络层添加适配器，QLORA避免了以前工作中观察到的几乎所有的准确性折衷。这种方法将65B参数模型的内存需求从>780GB降低到<48GB，使得在单个GPU上微调最大的公开可用模型成为可能。
- 性能结果：使用QLORA，研究者训练了Guanaco家族模型，其中指标第二的模型在Vicuna基准测试上达到了ChatGPT性能水平的97.8%，它在单个GPU上不到12小时内就可以完成培训。最大模型在单个GPU上超过24小时后达到了99.3%的性能水平，几乎追赶上了ChatGPT。在部署时，最小的Guanaco模型（7B参数）仅需要5GB内存，比26GB的Alpaca模型在Vicuna基准测试上表现更好，可以在下面的表6中看具体结果。

<img src='/images/lora_2.png'>

- 内存节省创新：QLORA引入了3项创新，用于减少内存使用，同时保持性能。这些创新有效地解决了内存问题，使得更多规模的模型微调成为可能，具体创新内容如下：
使用4位正态浮点：信息理论上最优的量化数据类型，适用于正态分布的数据。
双重量化：将量化常数进行量化的方法，平均每个参数节省约0.37位（对于65B模型约3GB）。
Paged Optimizers：使用NVIDIA统一内存来避免在处理长序列的小批次时出现梯度变量的内存峰值。
- 研究范围扩展：QLORA的高效性使作者能够进行更深入的研究，包括指令微调和聊天机器人性能，以及各种规模的模型训练。他们发现数据质量比数据集大小更为重要，并且强调了数据集的适用性对任务的性能更为关键。
- 质性分析：作者进行了对Guanaco模型的质性分析，突显了量化基准没有捕捉到的成功和失败案例。
- 资源共享：最后，作者开源了他们的模型、代码和方法，以促进进一步的研究，并将方法整合到Hugging Face 的transformers项目中，以便更多人可以访问。他们发布了多个适配器，用于不同规模的模型和数据集。

<img src='/images/lora_3.png'>

## 2. Background

 Block-wise k-bit Quantization：量化是将数据从一个表示更多信息的形式转换为一个表示较少信息的形式的过程。通常情况下，这涉及将数据类型从一个占用更多比特的形式转换为一个占用较少比特的形式，例如从32位浮点数转换为8位整数。为了确保较少比特的数据类型能够充分利用其范围，通常会对输入数据进行归一化处理，使其适应目标数据类型的范围。 例如，将FP32张量量化为范围为[-127, 127]的Int8张量：

<img src='/images/lora_4.png'>

其中 c 是量化常数或量化比例。反量化是其逆过程：

<img src='/images/lora_5.png'>

异常值处理：当输入张量中存在较大幅度的值时，传统的量化方法可能会导致某些量化区间未被充分利用。为了解决这个问题，提出了一种将输入张量分块处理的方法，每个块都有自己的量化常数 c 。具体如下：将输入张量 X∈Rb×h 分成大小为B 的 n 个连续块，然后将线性段切分为 n=(b×h)/B 个块，独立地对这些块进行量化，即使用等式1创建一个量化张量和 n 个量化常数 ci 。这种方法的目的是通过独立地处理每个块，避免了异常值的影响，并确保了所有量化区间都能够被充分利用。

Low-rank Adapters：低秩适配器（LoRA）微调是一种用于减少内存需求的方法，通过使用一小组可训练参数（称为适配器），并保持固定的完整模型参数，不对其进行更新。该方法利用随机梯度下降将梯度传递到适配器，以优化损失函数。LoRA通过引入额外的分解投影来增强线性投影。给定一个投影方程 XW=Y ，其中 ，X∈Rb×h，W∈Rh×o ，LoRA计算如下：

<img src='/images/lora_6.png'>


 Low-rank Adapters：低秩适配器（LoRA）微调是一种用于减少内存需求的方法，通过使用一小组可训练参数（称为适配器），并保持固定的完整模型参数，不对其进行更新。该方法利用随机梯度下降将梯度传递到适配器，以优化损失函数。LoRA通过引入额外的分解投影来增强线性投影。给定一个投影方程 XW=Y ，其中 ，X∈Rb×h，W∈Rh×o ，LoRA计算如下：


Parameter-Efficient微调的内容需求：LoRA内存需求的讨论涉及到适配器数量和大小，因为LoRA的内存占用较小，因此可以使用更多的适配器来提高性能。尽管LoRA被设计为参数高效微调（PEFT）方法，但LLM微调的大部分内存占用来自激活梯度，而不是学习到的LoRA参数。梯度检查点技术能够减少输入梯度的内存占用，但过度减少LoRA参数数量只会带来微小的内存优势。因此，可以使用更多的适配器，而不会显著增加整体训练内存占用量，这对于恢复完整的16位精度性能至关重要。




## 3. QLORA微调

 技术概述：QLORA通过提出的两种技术实现高保真的4比特微调，这两种技术是4比特NormalFloat（NF4）量化和双重量化。此外，QLORA引入了分页优化器，以防止梯度检查点期间的内存峰值导致的内存溢出错误，这些错误传统上使得在单机上微调大型模型变得困难。

 存储与计算数据类型：QLORA有一个低精度的存储数据类型，通常是4比特，和一个通常为BFloat16的计算数据类型。在实践中，这意味着每当使用QLORA的权重张量时，将张量反量化到BFloat16，然后执行16位的矩阵乘法。

### 3.1 位正态浮点量化

 量化基础 - 正态浮点（NF）数据类型： 正态浮点（NormalFloat, NF）数据类型是建立在分位数量化的基础上。分位数量化是一种信息论上的最优数据类型，它确保每个量化区间有相同数量的输入张量值。通过经验累积分布函数估算输入张量的分位数来实现。

分位数量化的局限性与近似算法： 分位数量化的主要局限在于估算过程的高成本。因此，采用了如SRAM分位数这样的快速近似算法来估算分位数。由于这些算法的近似性质，对于异常值（通常是最重要的值）会有较大的量化误差。

利用固定分布避免高成本估算和近似误差： 当输入张量来自于一个固定分布（仅在量化常数上有所不同）时，可以避免昂贵的分位数估算和近似误差。在这种情况下，输入张量有相同的分位数，使得精确的分位数估算在计算上可行。

预训练神经网络权重的分布转换： 预训练神经网络权重通常呈零中心正态分布，标准差为 σ 。通过缩放 σ ，可以将所有权重转换为单一固定分布，使分布完全符合数据类型的范围。对于该数据类型，设定了任意范围 [−1,1] 。因此，数据类型的分位数和神经网络权重都需要规范化到这个范围内。

针对正态分布的量化数据类型计算： 对于范围在 [−1,1] 内的零均值正态分布，其标准差为任意 σ 的信息论上最优数据类型的计算方式如下：

估算理论上的 N(0,1) 分布的 2k+1 个分位数，以获得 k 位的正态分布量化数据类型；
将此数据类型的值规范化到 [−1,1] 范围内；
通过绝对最大值重缩放将输入权重张量规范化到 [−1,1] 范围内进行量化。
步骤 (3) 相当于重新缩放权重张量的标准差，以匹配 k 位数据类型的标准差。更具体地，数据类型的 2k 个值 qi 的估算方式为： qi=12(QX(i2k+1)+QX(i+12k+1)) ，其中 QX(·) 是标准正态分布 N(0,1) 的分位数函数。

对称量化的问题与非对称数据类型的创建：

对于对称的 k 位量化，上述方法无法精确表示零，而零的精确表示对于量化填充和其他零值元素而无误差是重要的。为了确保一个离散的零点为0，并使用所有的 2k 位表示 k 位数据类型，

通过估算两个范围的分位数 ：qi：2(k−1) 用于负部分和 2(k−1)+1 用于正部分，然后将这些 qi 集合统一并移除两个集合中都出现的一个零。所得到的数据类型被称为 k 位正态浮点（NFk），因为该数据类型对于零中心的正态分布数据是信息论上最优的。这种数据类型的具体值可以在附录E中找到。




### 3.2 双重量化

 双重量化（DQ）的引入： 引入了双重量化（Double Quantization, DQ）的概念，这是一种对量化常数进行二次量化的过程，目的是为了进一步节省内存。尽管精确的4位量化需要小的块大小，但这也带来了相当大的内存开销。例如，使用32位的量化常数和64的块大小对于权重 W ，平均每个参数的量化常数增加了 32/64=0.5 位。

 双重量化的具体过程： 具体来说，双重量化将第一次量化的量化常数 c2FP32 作为第二次量化的输入。这一第二步产生了量化的量化常数 c2FP8 和第二层量化常数 c1FP32 。研究者使用256块大小的8位浮点数进行第二次量化，根据Dettmers和Zettlemoyer的研究，8位量化并没有观察到性能下降。由于 c2FP32 是正数，在量化之前从 c2 中减去均值以使值围绕零中心，并利用对称量化。

 内存占用的减少： 平均来说，对于64的块大小，这种量化方法将每个参数的内存占用从 32/64=0.5 位降低到 8/64+32/(64·256)=0.127 位，每个参数减少了0.373位的内存占用。

### 3.3 优化器状态分配分页内存

使用NVIDIA统一内存特性： 论文中提到了使用NVIDIA的统一内存（Unified Memory）特性，这个特性可以在GPU偶尔内存不足的情况下，自动在CPU和GPU之间进行页到页的传输，以保证GPU处理过程中无误差。这个特性类似于CPU RAM和硬盘之间的常规内存分页。

优化器状态的内存分配： 作者使用这个特性来为优化器状态分配分页内存。当GPU内存不足时，这些状态会自动被逐出到CPU RAM，然后在优化器更新步骤中需要内存时，再分页回GPU内存。




## 4. QLoRA vs.标准微调

 QLoRA工作原理与内存优化效果： 论文已经讨论了QLoRA是如何工作的，以及它如何显著减少微调模型所需的内存。QLoRA通过高效的量化策略和优化的内存管理，降低了微调过程中的内存需求。

 QLoRA性能分析与全模型微调对比： 目前的主要问题是QLoRA是否能够像全模型微调那样表现良好。这个问题涉及到量化模型的性能是否能够达到未量化模型的水平，尤其是在微调场景中。

 分析QLoRA的组成部分及其影响： 接下来的部分将分析QLoRA的各个组成部分，包括NormalFloat4与标准Float4相比的影响。这一分析将帮助理解QLoRA中每个组件的作用和重要性，以及它们如何共同作用以实现内存优化和性能保持。

 实验部分的讨论： 随后的章节将讨论旨在回答上述问题的实验。这些实验将提供关于QLoRA性能和效率的定量数据，帮助评估其在实际应用中的有效性和可行性。



## 5. 4位NormalFloat的优势

 4位NormalFloat的性能优势： 尽管从理论上讲，4位的NormalFloat（NF4）数据类型是信息论上最优的，但其是否能在实际应用中转化为明显的优势还需验证。论文遵循了Dettmers和Zettlemoyer的实验设置，评估了不同大小（从125M到65B）的量化大型语言模型（如OPT、BLOOM、Pythia、LLaMA）在语言建模和一系列零样本任务上的表现。

 不同数据类型性能比较： NF4在性能上相比于4位浮点数（FP4）和4位整数（Int4）有显著的提升。这表明NF4在保持数据压缩的同时，能够更好地保留模型的性能。

 双重量化对性能和内存占用的影响： 双重量化（Double Quantization）在减少内存占用的同时，并没有降低性能。这表明通过适当的量化策略，可以在减少模型大小和内存需求的同时，保持甚至提升模型的效果。

## 6. k-bitQLORA、16-bit全微调和16-bit LoRA性能

 4位量化对性能的影响： 先前的研究已经表明，4位量化用于推理是可行的，但相比于16位，会导致性能下降。这就引出了一个关键问题：通过进行4位适配器微调，是否能够恢复因不精确量化而丢失的性能。

 两种设置下的实验测试： 首先，论文对比了在GLUE和Super-NaturalInstructions数据集上对125M至3B参数规模的RoBERTA和T5模型进行全16位微调的结果。在这两个数据集上，观察到16位、8位和4位适配器方法均能复制全16位基线的性能。这表明通过适配器微调，可以完全恢复因不精确量化而丢失的性能。

 更大规模模型的性能测试： 由于在11B以上参数的模型全微调需要多台高内存GPU服务器，论文继续测试了在7B至65B参数规模上，4位QLoRA是否能匹配16位LoRA的性能。具体来说，对LLaMA 7B至65B在Alpaca和FLAN v2上进行微调，并在MMLU基准测试上通过5次射击准确度进行评估。结果显示，采用双重量化的NF4完全恢复了16位LoRA的MMLU性能。此外，还注意到，使用FP4的QLoRA比16位brain float LoRA基线低约1个百分点。

这进一步证实：

（1）使用NF4的QLoRA能复制16位全微调和16位LoRA微调的性能。
（2）在量化精度方面，NF4优于FP4。

## 7. 概括结果

 4位QLoRA的性能表现： 结果一致显示，使用4位NF4数据类型的QLoRA在学术基准测试中与16位全模型微调和16位LoRA微调的性能相匹配。这在具有良好评估设置的学术基准测试中得到了验证。
 NF4与FP4的比较： 实验还表明，NF4相比于FP4更为有效，并且双重量化并不会降低性能。这为4位QLoRA调优能可靠地匹配16位方法的结果提供了有力的证据。
 模型参数与精度的平衡： 与之前关于量化的工作一致，MMLU和Elo结果表明，在给定的微调和推理资源预算下，增加基础模型的参数数量同时降低其精度是有益的。这突显了QLoRA提供的效率优势的重要性。
 4位微调的性能-精度权衡： 在4位微调的实验中，没有观察到与全微调相比的性能下降，这引发了关于QLoRA调优中性能-精度权衡的问题。这一问题留待未来的研究去探索。
 未来的研究方向： 作者计划继续研究在完整16位微调在学术研究硬件上无法探索的规模的指令调优。