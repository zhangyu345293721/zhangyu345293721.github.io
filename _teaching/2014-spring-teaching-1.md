---
title: "LightGBM 的完整解释 - 最快的梯度提升模型"
collection: teaching
type: "lightGBM专栏"
permalink: /teaching/2014-spring-teaching-1
date: 2024-03-01
---

LightGBM是微软于2016年开发的梯度提升决策树模型（GBDT），与其他GBDT模型相比，LightGBM的最大特点是训练效率更快、准确率更高。

LightGBM 与一般的 Gradient Boosting Decision Tree 模型在结构上没有根本的区别，但通过以下特殊技术，LightGBM 使其训练速度更快。

1. 基于梯度的一侧采样（GOSS）
2. 树节点分裂中基于直方图的最佳值搜索
3. 分类特征的最佳分割
4. 独家功能捆绑
5. 叶向树生长策略
6. 并行优化

### 1.基于梯度的单侧采样（GOSS）

经典的基于树的梯度提升（GBDT）训练是一个重复过程，用于训练新树以适应所有训练集实例上先前树集的预测误差。（预测误差是所有训练集实例上的损失函数梯度）
因此，默认情况下，GBDT 使用所有训练集实例来训练其集合中的每棵树。
针对这一点，LightGBM引入了GOSS，其中我们只需要使用部分训练集来训练每个集成树。
1. 具有大梯度的训练实例意味着该实例具有较大的当前预测误差，并且应该是适合下一个集成树的主要目标
2. 小梯度的训练实例意味着该实例当前的预测误差相对较小，不需要下一个集成树过多担心，因此我们可以以某种概率丢弃它。
般来说，GOSS的主要思想是，在训练下一个集成树之前，我们保留梯度较大的训练实例，并丢弃一些梯度较小的训练实例。
下图为GOSS算法。
<br/><img src="/images/lgb_1.png">
所有训练实例均按梯度排序，a表示大梯度实例的采样百分比，b表示小梯度实例的采样百分比。
<br/><img src="/images/Lgb2.png">
通过使用 GOSS，我们实际上减少了训练下一个集成树的训练集的大小，这将使训练新树的速度更快。

### 2.基于直方图的树节点分裂
在寻找最佳特征值来分割树节点时，LightGBM使用特征值直方图，并尝试所有直方图bin值，而不是尝试所有可能的特征值，因此可以减少寻找最佳特征吐出值的时间和计算量。顺便说一下，LightGBM 的分割标准是减少从父级到子级的梯度方差。
例如，给定下面的年龄特征，将直方图离散特征值放入不同的范围箱中，因此我们可以使用像Age⩽30，Age⩽40，，，，Age⩽100这样的吐槽标准，而不是尝试像Age这样的所有可能的年龄值⩽31、年龄⩽32 等
<br/><img src="/images/lgb3.png"><br/>
用bin来替换原始数据相当于增加正则化，bin的数量决定了正则化的程度。bin 越小，惩罚越严重，欠拟合的风险越高。
>  1. 同样在树分裂场景中，对于给定的特征，直方图是可加的

父节点直方图 = 左子直方图 + 右子直方图
因此，在计算子直方图时，我们只需要计算一个子直方图（选择较小尺寸的子直方图），另一子直方图是父直方图减去计算得到的直方图。

### 3.分类特征的最优分割
通常，在处理树节点分裂中的分类特征时，我们总是使用One Vs Rest作为节点分裂规则，例如分裂条件是“Weather = Sunny” vs “All other Weather (Rainy, Cloudy, Snowy etc)”。一般来说，这一“一对一”策略的问题是
1. 它往往会在子节点中生成不平衡的数据点分配（例如左子节点比右子节点分配更多的数据点）并且需要增长得很深才能获得良好的准确性
2. 由于需要生长很深的树，需要多次节点分裂，所以建树效率很低。
受这些问题的启发，LightGBM 采用了如下多对多策略。

对于给定的分类特征
1. 对于特征的每个类别，计算平均值 Sum(y)/Count(y)
2. 按平均值对所有类别进行排序（如下图所示）。
3. 从最低平均值到最大平均值枚举分割值，以找到最佳分割值。分裂值将所有类别分为两部分（类别均值小于或大于分裂值），这就是节点分裂条件。
<br/><img src="/images/lgb4.png"><br/>

### 4.独家功能捆绑
EFB旨在通过合并特征来减少特征，具体来说就是合并互斥的特征，这些特征很少同时取非零值。

LightGBM提供了以下两种算法来实现

1. 从训练集中识别互斥的特征包
2. 合并功能包并为该包分配一个值
   
<br/><img src="/images/lgb5.png"><br/>
  
