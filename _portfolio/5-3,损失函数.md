---
title: "5-3,æŸå¤±å‡½æ•°losses"
excerpt: 'ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚(Objective = Loss + Regularization)
<br/><img src="/images/pyspark.png" width="600">'
collection: portfolio
---


ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚(Objective = Loss + Regularization)

Pytorchä¸­çš„æŸå¤±å‡½æ•°ä¸€èˆ¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶å€™æŒ‡å®šã€‚

æ³¨æ„Pytorchä¸­å†…ç½®çš„æŸå¤±å‡½æ•°çš„å‚æ•°å’Œtensorflowä¸åŒï¼Œæ˜¯y_predåœ¨å‰ï¼Œy_trueåœ¨åï¼Œè€ŒTensorflowæ˜¯y_trueåœ¨å‰ï¼Œy_predåœ¨åã€‚

å¯¹äºå›å½’æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„å†…ç½®æŸå¤±å‡½æ•°æ˜¯å‡æ–¹æŸå¤±å‡½æ•°nn.MSELoss ã€‚

å¯¹äºäºŒåˆ†ç±»æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°nn.BCELoss (è¾“å…¥å·²ç»æ˜¯sigmoidæ¿€æ´»å‡½æ•°ä¹‹åçš„ç»“æœ) 
æˆ–è€… nn.BCEWithLogitsLoss (è¾“å…¥å°šæœªç»è¿‡nn.Sigmoidæ¿€æ´»å‡½æ•°) ã€‚

å¯¹äºå¤šåˆ†ç±»æ¨¡å‹ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•° nn.CrossEntropyLossã€‚
(y_trueéœ€è¦æ˜¯ä¸€ç»´çš„ï¼Œæ˜¯ç±»åˆ«ç¼–ç ã€‚y_predæœªç»è¿‡nn.Softmaxæ¿€æ´»ã€‚) 

æ­¤å¤–ï¼Œå¦‚æœå¤šåˆ†ç±»çš„y_predç»è¿‡äº†nn.LogSoftmaxæ¿€æ´»ï¼Œå¯ä»¥ä½¿ç”¨nn.NLLLossæŸå¤±å‡½æ•°(The negative log likelihood loss)ã€‚
è¿™ç§æ–¹æ³•å’Œç›´æ¥ä½¿ç”¨nn.CrossEntropyLossç­‰ä»·ã€‚

å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_predï¼Œy_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

Pytorchä¸­çš„æ­£åˆ™åŒ–é¡¹ä¸€èˆ¬é€šè¿‡è‡ªå®šä¹‰çš„æ–¹å¼å’ŒæŸå¤±å‡½æ•°ä¸€èµ·æ·»åŠ ä½œä¸ºç›®æ ‡å‡½æ•°ã€‚

å¦‚æœä»…ä»…ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®ç°ç›¸åŒçš„æ•ˆæœã€‚



```python

```

## ä¸€ï¼Œå†…ç½®æŸå¤±å‡½æ•°

å†…ç½®çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰ç±»çš„å®ç°å’Œå‡½æ•°çš„å®ç°ä¸¤ç§å½¢å¼ã€‚

å¦‚ï¼šnn.BCE å’Œ F.binary_cross_entropy éƒ½æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå‰è€…æ˜¯ç±»çš„å®ç°å½¢å¼ï¼Œåè€…æ˜¯å‡½æ•°çš„å®ç°å½¢å¼ã€‚

å®é™…ä¸Šç±»çš„å®ç°å½¢å¼é€šå¸¸æ˜¯è°ƒç”¨å‡½æ•°çš„å®ç°å½¢å¼å¹¶ç”¨nn.Moduleå°è£…åå¾—åˆ°çš„ã€‚

ä¸€èˆ¬æˆ‘ä»¬å¸¸ç”¨çš„æ˜¯ç±»çš„å®ç°å½¢å¼ã€‚å®ƒä»¬å°è£…åœ¨torch.nnæ¨¡å—ä¸‹ï¼Œå¹¶ä¸”ç±»åä»¥Lossç»“å°¾ã€‚

å¸¸ç”¨çš„ä¸€äº›å†…ç½®æŸå¤±å‡½æ•°è¯´æ˜å¦‚ä¸‹ã€‚

* nn.MSELossï¼ˆå‡æ–¹è¯¯å·®æŸå¤±ï¼Œä¹Ÿå«åšL2æŸå¤±ï¼Œç”¨äºå›å½’ï¼‰

* nn.L1Loss ï¼ˆL1æŸå¤±ï¼Œä¹Ÿå«åšç»å¯¹å€¼è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼‰

* nn.SmoothL1Loss (å¹³æ»‘L1æŸå¤±ï¼Œå½“è¾“å…¥åœ¨-1åˆ°1ä¹‹é—´æ—¶ï¼Œå¹³æ»‘ä¸ºL2æŸå¤±ï¼Œç”¨äºå›å½’)

* nn.BCELoss (äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œè¾“å…¥å·²ç»è¿‡nn.Sigmoidæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)

* nn.BCEWithLogitsLoss (äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œè¾“å…¥æœªç»è¿‡nn.Sigmoidæ¿€æ´»)

* nn.CrossEntropyLoss (äº¤å‰ç†µï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥æœªç»è¿‡nn.Softmaxæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)

* nn.NLLLoss (è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥ç»è¿‡nn.LogSoftmaxæ¿€æ´»)

* nn.KLDivLoss (KLæ•£åº¦æŸå¤±ï¼Œä¹Ÿå«ç›¸å¯¹ç†µï¼Œç­‰äºäº¤å‰ç†µå‡å»ä¿¡æ¯ç†µï¼Œç”¨äºæ ‡ç­¾ä¸ºæ¦‚ç‡å€¼çš„å¤šåˆ†ç±»ï¼Œè¦æ±‚è¾“å…¥ç»è¿‡nn.LogSoftmaxæ¿€æ´»)

* nn.CosineSimilarity(ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¯ç”¨äºå¤šåˆ†ç±»)

* nn.AdaptiveLogSoftmaxWithLoss (ä¸€ç§é€‚åˆéå¸¸å¤šç±»åˆ«ä¸”ç±»åˆ«åˆ†å¸ƒå¾ˆä¸å‡è¡¡çš„æŸå¤±å‡½æ•°ï¼Œä¼šè‡ªé€‚åº”åœ°å°†å¤šä¸ªå°ç±»åˆ«åˆæˆä¸€ä¸ªcluster)

é‡ç‚¹ä»‹ç»ä¸€ä¸‹ äºŒå…ƒäº¤å‰ç†µã€å¤šå…ƒäº¤å‰ç†µã€å¯¹æ•°æŸå¤±LogLossã€è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±NLLLossã€KLæ•£åº¦ä¹‹é—´çš„åŒºåˆ«å’Œè”ç³»ã€‚

**1ï¼ŒäºŒåˆ†ç±»çš„äº¤å‰ç†µçš„è®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆæ˜¯è¿™æ ·ä¸€ç§å½¢å¼ï¼Ÿ**

$$
BinaryCrossEntropyLoss(Y,\hat{Y}) = - \frac{1}{N}\sum_{i=0}^{N-1} (y_i log \hat{y_i} + (1-y_i) log(1-\hat{y_i}))
$$


è¯¥å…¬å¼ç”±æå¤§ä¼¼ç„¶åŸç†æ¨å¯¼å¾—æ¥ã€‚ç”±äº $\hat{y_i}$è¡¨ç¤ºçš„æ˜¯æ ·æœ¬æ ‡ç­¾ä¸º1çš„æ¦‚ç‡ï¼Œ $1-\hat{y_i}$è¡¨ç¤ºçš„æ˜¯æ ·æœ¬æ ‡ç­¾ä¸º0çš„æ¦‚ç‡ï¼Œ
é‚£ä¹ˆè®­ç»ƒé›†ä¸­çš„å…¨éƒ¨æ ·æœ¬å–å¾—å¯¹åº”æ ‡ç­¾çš„æ¦‚ç‡å³ä¼¼ç„¶å‡½æ•°å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼

$$
L(Y,\hat{Y}) = \prod_{i=0}^{N-1} \hat{y_i}^{y_i} (1-\hat{y_i})^{(1-y_i)}
$$

æ³¨æ„å½“$y_i = 1$ä¸ºæ—¶ï¼Œè¿ä¹˜ä¸­çš„é¡¹ä¸º $\hat{y_i}$ï¼Œå½“$y_i = 0$ä¸ºæ—¶ï¼Œè¿ä¹˜ä¸­çš„é¡¹ä¸º $(1-\hat{y_i})$ã€


è½¬æ¢æˆå¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼Œå¾—åˆ° 

$$
lnL(Y,\hat{Y}) = \sum_{i=0}^{N-1} y_i ln{\hat{y_i}} + (1-y_i)ln{(1-\hat{y_i})} 
$$

å¯¹æ•°ä¼¼ç„¶å‡½æ•°æ±‚æå¤§å€¼ï¼Œç­‰ä»·äºå¯¹å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„è´Ÿæ•°æ±‚æå°å€¼ï¼Œè€ƒè™‘æ ·æœ¬æ•°é‡ç»´åº¦å½’ä¸€åŒ–ï¼Œäºæ˜¯å¾—åˆ°äº†äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°çš„å½¢å¼ã€‚





**2ï¼Œå¤šå…ƒäº¤å‰ç†µçš„è®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿå’ŒäºŒå…ƒäº¤å‰ç†µæœ‰ä»€ä¹ˆè”ç³»?**


$$
CrossEntropyLoss(Y,\hat{Y}) = - \frac{1}{N}\sum_{i=0}^{N-1} \sum_{k=0}^{K-1} I(y_i==k) log \hat{y_{i,k}} \\
\text{where} I(x) \text{ is the Indicator function} \\
I(True)= 1 \text{ and } I(False) = 0
$$

å¤šå…ƒäº¤å‰ç†µæ˜¯äºŒå…ƒäº¤å‰ç†µçš„è‡ªç„¶æ‹“å±•ï¼Œå…¶ä¸­$y_i$å–0~K-1å…¶ä¸­çš„ä¸€ä¸ªç±»åˆ«ç¼–ç åºå·ï¼Œ$\hat{y_i}$ æ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºKçš„æ¦‚ç‡å‘é‡ã€‚å¤šå…ƒäº¤å‰ç†µçš„ç±»åˆ«æ•°Kå–2æ—¶å³å¯å¾—åˆ°äºŒå…ƒäº¤å‰ç†µå¯¹åº”çš„å…¬å¼ã€‚


**3ï¼Œsklearnï¼Œcatboostç­‰åº“ä¸­å¸¸å¸¸çœ‹åˆ°loglosså¯¹æ•°æŸå¤±å‡½æ•°ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°å¦‚ä½•è®¡ç®—ï¼Œå’Œäº¤å‰ç†µæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ**

$$
LogLoss(Y,\hat{Y}) = - \frac{1}{N}\sum_{i=0}^{N-1}  log(\hat{y_{i}}[y_i])
$$

å…¬å¼ä¸­çš„æ–¹æ‹¬å·å’ŒPythonä¸­çš„ç´¢å¼•çš„ç”¨æ³•ä¸€è‡´ï¼Œè¡¨ç¤ºå–$\hat{y_{i}}$ çš„ç¬¬$y_i$ä¸ªå…ƒç´ ã€‚

å®¹æ˜“è¯æ˜ï¼Œå¯¹æ•°æŸå¤±å‡½æ•°ä¸äº¤å‰ç†µå‡½æ•°å®Œå…¨ç­‰ä»·ï¼Œæ˜¯äº¤å‰ç†µçš„å¦å¤–ä¸€ç§è§†è§’: å³æ¯ä¸ªæ ·æœ¬å¯¹å…¶æ ‡ç­¾å¯¹åº”ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å€¼æ±‚å¯¹æ•°ï¼Œæ±‚å¹³å‡å†å–è´Ÿæ•°å³å¯ã€‚


**4ï¼Œpytorchä¸­çš„ nn.NLLLoss å’Œ nn.CrossEntropyLossæœ‰ä»€ä¹ˆåŒºåˆ«å’Œè”ç³»ï¼Ÿ**

NLLoss å…¨ç§°æ˜¯ Negative Log Likelihood Lossï¼Œå³ è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚å…¶è®¡ç®—å…¬å¼å¦‚ä¸‹

$$
NLLoss(Y,\hat{Z}) = - \frac{1}{N}\sum_{i=0}^{N-1}  {z_{i}}[y_i]
$$

å…¬å¼ä¸­çš„æ–¹æ‹¬å·å’ŒPythonä¸­çš„ç´¢å¼•çš„ç”¨æ³•ä¸€è‡´ï¼Œè¡¨ç¤ºå–$\hat{z_{i}}$ çš„ç¬¬$y_i$ä¸ªå…ƒç´ ã€‚

æ³¨æ„çš„æ˜¯è¿™é‡Œçš„$\hat{Z}$å®é™…ä¸Šä¸æ˜¯æ¦‚ç‡å€¼ï¼Œè€Œæ˜¯æ¦‚ç‡å€¼å–äº†å¯¹æ•°ï¼Œæ‰€ä»¥ï¼Œå’ŒLogLossä¸€å¯¹æ¯”ï¼Œå¾ˆå®¹æ˜“å‘ç°ï¼ŒLogSoftmax+NLLLoss ç­‰ä»·äº Softmax+LogLoss,ç­‰ä»·äº Softmax+CrossEntropyLossã€‚ä¸ºäº†æ•°å€¼ç²¾åº¦è€ƒè™‘ï¼Œpytorchä¸­çš„nn.CrossEntropyLossè¦æ±‚è¾“å…¥æœªç»è¿‡Softmaxæ¿€æ´»ï¼Œæ‰€ä»¥æœ‰ nn.LogSoftmax+nn.NLLLoss ç­‰ä»·äº nn.CrossEntropyLoss.


**5ï¼ŒKLæ•£åº¦çš„è®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆç°å®å«ä¹‰ï¼Ÿå’Œäº¤å‰ç†µæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ**

KLæ•£åº¦ä¹Ÿå«ç›¸å¯¹ç†µï¼Œå¯ä»¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚

KLæ•£åº¦çš„è®¡ç®—å…¬å¼æ˜¯äº¤å‰ç†µå‡å»ä¿¡æ¯ç†µã€‚æ³¨æ„KLæ•£åº¦æ˜¯ä¸å¯¹ç§°çš„, å³$KL(P,Q)\neq KL(Q,P)$, æ‰€ä»¥ä¸èƒ½å¤Ÿå«åšKLè·ç¦»ã€‚

ä¸¤ä¸ªéšæœºå˜é‡På’ŒQä¹‹é—´çš„KLæ•£åº¦å®šä¹‰å¦‚ä¸‹ï¼š
$$
KL(P,Q) = \sum_{k=0}^{K-1}p_k ln(\frac{p_k}{q_k}) = \sum_{k=0}^{K-1} p_k (ln{p_k} - ln{q_k})
$$

å¯¹äºŒåˆ†ç±»æƒ…å†µä¸‹ï¼Œæœ‰ï¼š

$$
KL(Y,\hat{Y}) = - \frac{1}{N}\sum_{i=0}^{N-1} (y_i log \hat{y_i} + (1-y_i) log(1-\hat{y_i}))  + \frac{1}{N}\sum_{i=0}^{N-1} (y_i log y_i + (1-y_i) log(1- y_i))
$$

åœ¨$y_i$å–0æˆ–1çš„æƒ…å†µä¸‹ï¼Œä¿¡æ¯ç†µéƒ¨åˆ†ä¸º0ï¼Œæ‰€ä»¥KLæ•£åº¦å°±ç­‰äºäº¤å‰ç†µï¼Œä½†æ˜¯åœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ä½¿ç”¨æ ‡ç­¾å¹³æ»‘å¤„ç†æŠ€æœ¯åï¼Œ$y_i$çš„å–å€¼ä¸æ˜¯0æˆ–1ï¼Œè¿™æ—¶å€™ï¼ŒKLæ•£åº¦ç›¸å½“äºåœ¨äº¤å‰ç†µçš„åŸºç¡€ä¸Šå‡å»äº†ä¸€ä¸ªå¸¸æ•°ï¼ŒKLæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°å»ä¼˜åŒ–æ¨¡å‹çš„æ•ˆæœå’Œäº¤å‰ç†µæ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹å½“æ¨¡å‹å®Œç¾æ‹Ÿåˆæ ‡ç­¾çš„æƒ…å†µä¸‹KLæ•£åº¦çš„æœ€å°å€¼å¯å–åˆ°0ï¼Œè€Œæ­¤æ—¶äº¤å‰ç†µèƒ½å¤Ÿå–åˆ°çš„æœ€å°å€¼æ˜¯ä¿¡æ¯ç†µä¸ä¸º0ï¼Œæ‰€ä»¥è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨KLæ•£åº¦æ›´ç¬¦åˆæˆ‘ä»¬å¯¹Lossçš„ä¸€èˆ¬è®¤è¯†ã€‚



```python

```


```python
import numpy as np
import pandas as pd
import torch 
from torch import nn 
import torch.nn.functional as F 

# nn.BCELoss() å’Œ nn.BCEWithLogitsLoss() å…³ç³»

y_pred = torch.tensor([5.0,3,10,-5,-3,-10.0])
y_true = torch.tensor([1.0,1,1,0,0,0])

bce = nn.BCELoss()(torch.sigmoid(y_pred),y_true)
print(bce)


bce_logits = nn.BCEWithLogitsLoss()(y_pred,y_true)
print(bce_logits)

```

    tensor(0.0184)
    tensor(0.0184)

```python
# nn.CrossEntropyLoss() å’Œ  nn.NLLLoss() å…³ç³»

y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]])
y_true = torch.tensor([0,2])

# ç›´æ¥è°ƒç”¨äº¤å‰ç†µæŸå¤±
ce = nn.CrossEntropyLoss()(y_pred,y_true)
print(ce)

# ç­‰ä»·äºå…ˆè®¡ç®—nn.LogSoftmaxæ¿€æ´»ï¼Œå†è°ƒç”¨nn.NLLLoss
y_pred_logsoftmax = nn.LogSoftmax(dim = 1)(y_pred)
nll = nn.NLLLoss()(y_pred_logsoftmax,y_true)
print(nll)

```

    tensor(0.5493)
    tensor(0.5493)



```python
# nn.CrossEntropyLoss() å’Œ  KLDivLoss å…³ç³»
import torch.nn.functional as F 

y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]],requires_grad=True)
y_true = torch.tensor([0,2])

ce = nn.CrossEntropyLoss(reduction="mean")(y_pred,y_true)
print(ce)


#KLDivLossè¦æ±‚targetä¸ºå‘é‡å½¢å¼ç¼–ç ä¸”predsç»è¿‡LogSoftmaxæ¿€æ´»
pred = F.log_softmax(y_pred,dim=1)
target = F.one_hot(y_true).float()
kl = nn.KLDivLoss(reduction="batchmean")(pred,target)
print(kl)
```

    tensor(0.5493, grad_fn=<NllLossBackward0>)
    tensor(0.5493, grad_fn=<DivBackward0>)



```python

```

## äºŒï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°

è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_pred,y_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

ä¹Ÿå¯ä»¥å¯¹nn.Moduleè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™forwardæ–¹æ³•å®ç°æŸå¤±çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°æŸå¤±å‡½æ•°çš„ç±»çš„å®ç°ã€‚

ä¸‹é¢æ¼”ç¤ºä¸¤ä¸ªæ¯”è¾ƒè‘—åçš„èŒƒä¾‹ã€‚

### 1ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¹‹FocalLossèŒƒä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ªFocal Lossçš„è‡ªå®šä¹‰å®ç°ç¤ºèŒƒã€‚Focal Lossæ˜¯ä¸€ç§å¯¹binary_crossentropyçš„æ”¹è¿›æŸå¤±å‡½æ•°å½¢å¼ã€‚

å®ƒåœ¨æ ·æœ¬ä¸å‡è¡¡å’Œå­˜åœ¨è¾ƒå¤šæ˜“åˆ†ç±»çš„æ ·æœ¬æ—¶ç›¸æ¯”binary_crossentropyå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

å®ƒæœ‰ä¸¤ä¸ªå¯è°ƒå‚æ•°ï¼Œalphaå‚æ•°å’Œgammaå‚æ•°ã€‚å…¶ä¸­alphaå‚æ•°ä¸»è¦ç”¨äºè¡°å‡è´Ÿæ ·æœ¬çš„æƒé‡ï¼Œgammaå‚æ•°ä¸»è¦ç”¨äºè¡°å‡å®¹æ˜“è®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚

ä»è€Œè®©æ¨¡å‹æ›´åŠ èšç„¦åœ¨æ­£æ ·æœ¬å’Œå›°éš¾æ ·æœ¬ä¸Šã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸ªæŸå¤±å‡½æ•°å«åšFocal Lossã€‚

è¯¦è§ã€Š5åˆ†é’Ÿç†è§£Focal Lossä¸GHMâ€”â€”è§£å†³æ ·æœ¬ä¸å¹³è¡¡åˆ©å™¨ã€‹

https://zhuanlan.zhihu.com/p/80594704

$$focal\_loss(y,p) = 
\begin{cases} -\alpha (1-p)^{\gamma}\log(p) & \text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) & \text{if y = 0} 
\end{cases}$$


```python
import torch 
from torch import nn 
class FocalLoss(nn.Module):
    
    def __init__(self,gamma=2.0,alpha=0.75):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self,y_pred,y_true):
        bce = torch.nn.BCELoss(reduction = "none")(y_pred,y_true)
        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))
        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)
        modulating_factor = torch.pow(1.0 - p_t, self.gamma)
        loss = torch.mean(alpha_factor * modulating_factor * bce)
        return loss
    
    
    
```


```python
#å›°éš¾æ ·æœ¬
y_pred_hard = torch.tensor([[0.5],[0.5]])
y_true_hard = torch.tensor([[1.0],[0.0]])

#å®¹æ˜“æ ·æœ¬
y_pred_easy = torch.tensor([[0.9],[0.1]])
y_true_easy = torch.tensor([[1.0],[0.0]])

focal_loss = FocalLoss()
bce_loss = nn.BCELoss()


print("focal_loss(easy samples):", focal_loss(y_pred_easy,y_true_easy))
print("bce_loss(easy samples):", bce_loss(y_pred_easy,y_true_easy))

print("focal_loss(hard samples):", focal_loss(y_pred_hard,y_true_hard))
print("bce_loss(hard samples):", bce_loss(y_pred_hard,y_true_hard))


#å¯è§ focal_lossè®©å®¹æ˜“æ ·æœ¬çš„æƒé‡è¡°å‡åˆ°åŸæ¥çš„ 0.0005/0.1054 = 0.00474
#è€Œè®©å›°éš¾æ ·æœ¬çš„æƒé‡åªè¡°å‡åˆ°åŸæ¥çš„ 0.0866/0.6931=0.12496

# å› æ­¤ç›¸å¯¹è€Œè¨€ï¼Œfocal_losså¯ä»¥è¡°å‡å®¹æ˜“æ ·æœ¬çš„æƒé‡ã€‚

```

    focal_loss(easy samples): tensor(0.0005)
    bce_loss(easy samples): tensor(0.1054)
    focal_loss(hard samples): tensor(0.0866)
    bce_loss(hard samples): tensor(0.6931)


FocalLossçš„ä½¿ç”¨å®Œæ•´èŒƒä¾‹å¯ä»¥å‚è€ƒä¸‹é¢ä¸­`è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹`ä¸­çš„èŒƒä¾‹ï¼Œè¯¥èŒƒä¾‹æ—¢æ¼”ç¤ºäº†è‡ªå®šä¹‰æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ï¼Œä¹Ÿæ¼”ç¤ºäº†FocalLossçš„ä½¿ç”¨æ–¹æ³•ã€‚


### 2ï¼ŒSCELoss 

Symmetric Cross Entropy Loss ä¹Ÿæ˜¯ä¸€ç§å¯¹äº¤å‰ç†µæŸå¤±çš„æ”¹è¿›æŸå¤±ï¼Œä¸»è¦ç”¨åœ¨æ ‡ç­¾ä¸­å­˜åœ¨æ˜æ˜¾å™ªå£°çš„åœºæ™¯ã€‚



$$
sce\_loss(y,p) = \alpha\;ce\_loss(y,p) + \beta\;rce\_loss(y,p)\\
ce\_loss(y,p) = - y log(p) -(1-y) log(1-p) \\
rce\_loss(y,p) = ce\_loss(p,y)  \\
rce\_loss(y,p)= - p log(y) -(1-p) log(1-y) 
$$

å…¶åŸºæœ¬æ€æƒ³å¯ä»¥ç®€å•æè¿°å¦‚ä¸‹ï¼š

å½“ yæ˜¯æ­£å¸¸æ ‡ç­¾çš„æ—¶å€™ï¼Œyå’Œpè¾ƒå®¹æ˜“å–å¾—ä¸€è‡´ã€ä¾‹å¦‚ y=1æ—¶ï¼Œpå–åˆ°0.8ã€‘ï¼Œè¿™æ—¶å€™ rceä¸ceçš„æ¯”å€¼ç›¸å¯¹è¾ƒå¤§ï¼Œå¼•å…¥rceå¯ä»¥å¢åŠ æ­£å¸¸æ ‡ç­¾æ ·æœ¬åœ¨æ€»Lossä¸­çš„è´¡çŒ®ã€‚

å½“yæ—¶å™ªå£°æ ‡ç­¾çš„æ—¶å€™ï¼Œyå’Œpå¾ˆéš¾å–å¾—ä¸€è‡´ï¼Œç›¸å½“äºå›°éš¾æ ·æœ¬ ã€ä¾‹å¦‚ y=0æ—¶ï¼Œpå–åˆ°0.8ã€‘ï¼Œè¿™æ—¶å€™rceä¸ceçš„æ¯”å€¼ç›¸å¯¹è¾ƒå°ï¼Œå¼•å…¥rceå¯ä»¥å‡å°å™ªå£°æ ‡ç­¾æ ·æœ¬åœ¨æ€»Lossä¸­çš„è´¡çŒ®ã€‚



å‚è€ƒæ–‡ç« 

ã€ŠSCE æŸå¤±ã€‹ https://zhuanlan.zhihu.com/p/420827592

ã€Šå™ªå£°æŸå¤± ã€‹https://zhuanlan.zhihu.com/p/420913134



```python
def ce(y,p):
    p = torch.clamp(p,min=1e-4,max=1-1e-4)
    y = torch.clamp(y,min=1e-4,max=1-1e-4)
    return -y*torch.log(p) - (1-y)*torch.log(1-p)

def rce(y,p):
    return ce(p,y)

#æ­£å¸¸æ ‡ç­¾
y = torch.tensor(1.0)
p = torch.tensor(0.8)
print(rce(y,p)/ce(y,p))


#å™ªå£°æ ‡ç­¾
y = torch.tensor(0.0)
p = torch.tensor(0.8)
print(rce(y,p)/ce(y,p))

```

    tensor(8.2502)
    tensor(4.5786)



```python
import torch 
from torch import nn
import  torch.nn.functional as F 

class SCELoss(nn.Module):
    def __init__(self, num_classes=10, a=1, b=1):
        super(SCELoss, self).__init__()
        self.num_classes = num_classes
        self.a = a #ä¸¤ä¸ªè¶…å‚æ•°
        self.b = b
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(self, pred, labels):
        # CE éƒ¨åˆ†ï¼Œæ­£å¸¸çš„äº¤å‰ç†µæŸå¤±
        ce = self.cross_entropy(pred, labels)

        # RCE
        pred = F.softmax(pred, dim=1)
        pred = torch.clamp(pred, min=1e-4, max=1.0)
        label_one_hot = F.one_hot(labels, self.num_classes).float().to(pred.device)
        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0) #æœ€å°è®¾ä¸º 1e-4ï¼Œå³ A å– -4
        rce = (-1 * torch.sum(pred * torch.log(label_one_hot), dim=1))

        loss = self.a * ce + self.b * rce.mean()
        return loss
    
```


```python

```


```python

```

## ä¸‰ï¼ŒL1å’ŒL2æ­£åˆ™åŒ–é¡¹

L1æ­£åˆ™ã€L2æ­£åˆ™ã€Dropoutã€Early_stoppingæ˜¯ç¥ç»ç½‘ç»œå¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚




1ï¼ŒL1æ­£åˆ™å’ŒL2æ­£åˆ™çš„æ•ˆæœæœ‰ä»€ä¹ˆå·®å¼‚?ä¸ºä»€ä¹ˆï¼Ÿ


é€šå¸¸è®¤ä¸ºL1 æ­£åˆ™åŒ–å¯ä»¥äº§ç”Ÿç¨€ç–æƒå€¼çŸ©é˜µï¼Œå³äº§ç”Ÿä¸€ä¸ªå‚æ•°ç¨€ç–çš„æ¨¡å‹ã€‚

è€ŒL2 æ­£åˆ™åŒ–å¯ä»¥è®©æ¨¡å‹çš„å‚æ•°å–ç»å¯¹å€¼è¾ƒå°çš„æ•°ã€‚


è€ƒè™‘ä¸¤ç§æ­£åˆ™åŒ–å‡½æ•°çš„ç­‰å€¼é¢ä¸åŸå§‹Losså‡½æ•°çš„ç­‰å€¼é¢çš„å…³ç³»ã€‚

ä»¥äºŒç»´æƒ…å†µä¸ºä¾‹ï¼ŒL1æ­£åˆ™åŒ–å‡½æ•°çš„ç­‰å€¼é¢æ˜¯ä¸ªè±å½¢ï¼ŒL2æ­£åˆ™åŒ–å‡½æ•°çš„ç­‰å€¼é¢æ˜¯ä¸ªåœ†å½¢ã€‚

æœ€ä¼˜å‚æ•°å¿…å®šå–åœ¨æ­£åˆ™åŒ–å‡½æ•°çš„æŸæ¡ç­‰å€¼é¢å’ŒåŸå§‹Losså‡½æ•°çš„æŸæ¡ç­‰å€¼é¢çš„åˆ‡ç‚¹å¤„ã€‚

ä»æ±‚å¯¼è§’åº¦è€ƒè™‘ï¼Œæœ€ä¼˜å‚æ•°æ˜¯ä¸ªæå€¼ç‚¹ï¼Œè¦æ±‚è¯¥å¤„ æ­£åˆ™åŒ–å‡½æ•°çš„æ¢¯åº¦ç­‰äº åŸå§‹Losså‡½æ•°çš„æ¢¯åº¦çš„è´Ÿæ•°ã€‚

è€Œæ¢¯åº¦æ–¹å‘å¿…å®šå‚ç›´äºç­‰å€¼é¢çš„åˆ‡çº¿æ–¹å‘ï¼Œæ‰€ä»¥å¯ä»¥æ¨æ–­å¿…å®šæå€¼ç‚¹å¿…å®šåœ¨æ­£åˆ™åŒ–å‡½æ•°æŸæ¡ç­‰å€¼é¢å’ŒåŸå§‹Losså‡½æ•°çš„æŸæ¡ç­‰å€¼é¢çš„åˆ‡ç‚¹å¤„ã€‚

ä»æ•°å€¼è§’åº¦è€ƒè™‘ï¼Œå¦‚æœè¯¥æå€¼ç‚¹ä¸åœ¨ä¸¤ä¸ªç­‰å€¼é¢çš„åˆ‡ç‚¹ï¼Œé‚£ä¹ˆæ²¿ç€åŸå§‹å‡½æ•°Lossçš„ç­‰å€¼é¢(åŸå§‹Lossä¸å˜)ï¼Œä¸€å®šå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªç‚¹æ­£åˆ™åŒ–å‡½æ•°å–å€¼æ›´å°ã€‚

è¿™æ ·å°±ç”¨åè¯æ³•è¯æ˜äº†æœ€ä¼˜å‚æ•°å¿…å®šå–åœ¨æ­£åˆ™åŒ–å‡½æ•°çš„æŸæ¡ç­‰å€¼é¢å’ŒåŸå§‹Losså‡½æ•°çš„æŸæ¡ç­‰å€¼é¢çš„åˆ‡ç‚¹å¤„ã€‚

ç”±äºL1æ­£åˆ™åŒ–å‡½æ•°çš„ç­‰å€¼é¢æ˜¯ä¸ªè±å½¢ï¼Œæ›´å®¹æ˜“å’Œå‡¸çš„Losså‡½æ•°çš„ç­‰å€¼é¢ç›¸åˆ‡åœ¨åæ ‡è½´ä¸Šï¼Œæ‰€ä»¥å€¾å‘äºå–å¾—å‚æ•°ç¨€ç–çš„æ¨¡å‹ï¼Œè€ŒL2æ­£åˆ™åŒ–åˆ™æ›´å€¾å‘äºä½¿å¾—æå°ç‚¹åˆ°åæ ‡åŸç‚¹çš„è·ç¦»æ›´è¿‘ï¼Œä½†ä¸ä¼šå¯¼è‡´å‚æ•°ç¨€ç–ã€‚




![](https://tva1.sinaimg.cn/large/e6c9d24egy1h5q2vhkvz9j20pa0ctjsg.jpg)





å‚è€ƒæ–‡ç« ã€ŠL1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ–ã€‹ï¼šhttps://zhuanlan.zhihu.com/p/35356992 




```python
import torch 
# L2æ­£åˆ™åŒ–
def L2Loss(model,alpha):
    l2_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name: #ä¸€èˆ¬ä¸å¯¹åç½®é¡¹ä½¿ç”¨æ­£åˆ™
            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))
    return l2_loss

# L1æ­£åˆ™åŒ–
def L1Loss(model,beta):
    l1_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name:
            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))
    return l1_loss


```


## å››ï¼ŒL1L2æ­£åˆ™é¡¹ä½¿ç”¨å®Œæ•´èŒƒä¾‹

ä¸‹é¢ä»¥ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºç»™æ¨¡å‹çš„ç›®æ ‡å‡½æ•°æ·»åŠ è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ã€‚

è¿™ä¸ªèŒƒä¾‹åŒæ—¶æ¼”ç¤ºäº†ä»¥ä¸‹FocalLossçš„ä½¿ç”¨ã€‚

**1ï¼Œå‡†å¤‡æ•°æ®**


```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras 
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 1000,6000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);

```




```python
ds = TensorDataset(X,Y)

ds_train,ds_val = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_val = DataLoader(ds_val,batch_size = 100,num_workers=2)

features,labels = next(iter(dl_train))

```

**2ï¼Œå®šä¹‰æ¨¡å‹**


```python

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = self.fc3(x)
        return y
        
net = Net() 

from torchkeras import summary

summary(net,features);

```

    --------------------------------------------------------------------------
    Layer (type)                            Output Shape              Param #
    ==========================================================================
    Linear-1                                     [-1, 4]                   12
    Linear-2                                     [-1, 8]                   40
    Linear-3                                     [-1, 1]                    9
    ==========================================================================
    Total params: 61
    Trainable params: 61
    Non-trainable params: 0
    --------------------------------------------------------------------------
    Input size (MB): 0.000069
    Forward/backward pass size (MB): 0.000099
    Params size (MB): 0.000233
    Estimated Total Size (MB): 0.000401
    --------------------------------------------------------------------------



```python

```

**3ï¼Œè®­ç»ƒæ¨¡å‹**


```python
# L2æ­£åˆ™åŒ–
def L2Loss(model,alpha):
    l2_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name: #ä¸€èˆ¬ä¸å¯¹åç½®é¡¹ä½¿ç”¨æ­£åˆ™
            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))
    return l2_loss

# L1æ­£åˆ™åŒ–
def L1Loss(model,beta):
    l1_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name:
            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))
    return l1_loss

```


```python
from torchkeras import KerasModel
from torchkeras.metrics import AUC

net = Net()

# å°†L2æ­£åˆ™å’ŒL1æ­£åˆ™æ·»åŠ åˆ°FocalLossæŸå¤±ï¼Œä¸€èµ·ä½œä¸ºç›®æ ‡å‡½æ•°
def focal_loss_with_regularization(y_pred,y_true):
    y_probs = torch.sigmoid(y_pred)
    focal = FocalLoss()(y_probs,y_true) 
    l2_loss = L2Loss(net,0.001) #æ³¨æ„è®¾ç½®æ­£åˆ™åŒ–é¡¹ç³»æ•°
    l1_loss = L1Loss(net,0.001)
    total_loss = focal + l2_loss + l1_loss
    return total_loss


optimizer = torch.optim.Adam(net.parameters(),lr = 0.002)
model = KerasModel(net=net,
                   loss_fn = focal_loss_with_regularization ,
                   metrics_dict = {"auc":AUC()},
                   optimizer= optimizer )


dfhistory = model.fit(train_data=dl_train,
      val_data=dl_val,
      epochs=20,
      ckpt_path='checkpoint',
      patience=3,
      monitor='val_auc',
      mode='max',
      plot=True,
      cpu=True
    )
```

    [0;31m<<<<<< ğŸŒ cpu is used >>>>>>[0m



<style>
    /* background: */
    progress::-webkit-progress-bar {background-color: #CDCDCD; width: 100%;}
    progress {background-color: #CDCDCD;}

    /* value: */
    progress::-webkit-progress-value {background-color: #00BFFF  !important;}
    progress::-moz-progress-bar {background-color: #00BFFF  !important;}
    progress {color: #00BFFF ;}

    /* optional */
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #000000;
    }
</style>





<div>
  <progress value='20' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100% [20/20] [00:54]
  <br>
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ100.00% [21/21] [val_loss=0.0276, val_auc=0.9819]
</div>




```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(torch.sigmoid(net.forward(X))>=0.5)]
Xn_pred = X[torch.squeeze(torch.sigmoid(net.forward(X))<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");

```


```python

```

## äº”ï¼Œé€šè¿‡ä¼˜åŒ–å™¨å®ç°L2æ­£åˆ™åŒ–

å¦‚æœä»…ä»…éœ€è¦ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®ç°ã€‚

weight_decayå‚æ•°å¯ä»¥è®¾ç½®å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡°å‡ï¼Œè¿™å’ŒL2æ­£åˆ™åŒ–çš„ä½œç”¨æ•ˆæœç­‰ä»·ã€‚

```
before L2 regularization:

gradient descent: w = w - lr * dloss_dw 

after L2 regularization:

gradient descent: w = w - lr * (dloss_dw+beta*w) = (1-lr*beta)*w - lr*dloss_dw

so ï¼ˆ1-lr*betaï¼‰is the weight decay ratio.
```

Pytorchçš„ä¼˜åŒ–å™¨æ”¯æŒä¸€ç§ç§°ä¹‹ä¸ºPer-parameter optionsçš„æ“ä½œï¼Œå°±æ˜¯å¯¹æ¯ä¸€ä¸ªå‚æ•°è¿›è¡Œç‰¹å®šçš„å­¦ä¹ ç‡ï¼Œæƒé‡è¡°å‡ç‡æŒ‡å®šï¼Œä»¥æ»¡è¶³æ›´ä¸ºç»†è‡´çš„è¦æ±‚ã€‚


```python
weight_params = [param for name, param in model.named_parameters() if "bias" not in name]
bias_params = [param for name, param in model.named_parameters() if "bias" in name]

optimizer = torch.optim.SGD([{'params': weight_params, 'weight_decay':1e-5},
                             {'params': bias_params, 'weight_decay':0}],
                            lr=1e-2, momentum=0.9)

```
